
HOW SOCCER PLAYERS WOULD DO STREAM JOINS

  [ You can obtain an HTML version of this file by running doxygen
    with the `doxygen.conf' file in this directory.  The documentation
    will then be generated in doc/html/index.html. ]

A. Compilation (or Even Installation)

This is a prototype implementation to experiment with ``Handshake
Join,'' an algorithm proposed in our SIGMOD 2011 paper. The
implementation includes the join operator itself, but also the driver
code to run the experimental setup described in the paper.

The code has been written using standard GNU tools and uses autotools
for configuration. Thus, compilation should be as simple as:

     ./configure --disable-assert
     make

(You can run make install, too, if you really want to write the binary
somewhere in your system.)

Besides the usual ./configure options, the only really interesting
option that you can play with is --enable-simd, which enables the SIMD
optimizations discussed in the paper.

Our code makes use of the following libraries if they can be found on
your system:

 - POSIX real-time functionality (librt). Most importantly, we use the
   clock_nanosleep() from the POSIX real-time library. If it cannot be
   found, we use gettimeofday() instead, but performance may suffer from
   that.

 - libnuma. To place threads and data properly on NUMA systems, we make
   use of the libnuma library. Please make sure the library is installed
   in your system and can be found by ./configure. Again, the code will
   run also without libnuma, but performance may suffer, particularly on
   large NUMA systems.

We have successfully compiled and run our code on different Linux
variants; the experiments in the paper were performed on a Debian
system. Most of the code was developed on Mac OS X Snow Leopard, though
neither real-time functionality nor libnuma are available there.

B. Usage

B.1 Invocation

The sj binary understands the following command line options:

  -h          prints a little help screen
  -c NUM      configures the system to use NUM cores/threads for
              processing.  At runtime there will be two additional
              threads: a join driver that pushes input data into
              handshake join and a result collector that collects and
              counts all result tuples.
  -d PREFIX   To run the experiment, two random input streams will be
              generated. With this command line option, the content of
              both streams will be printed to two files named [PREFIX].R
              and [PREFIX].S, respectively. Both dump files can be used
              to compute the join result off-line and validate
              correctness.
  -l FILE     prints logging/debugging information to a file named FILE.
  -n NUM      number of input tuples to generate for input stream R.
  -N NUM      number of input tuples to generate for input stream S.
  -o FILE     When running in ``GUI mode'', print information about
              individual cores' state to FILE. A GUI can read this
              information and show a graphical representation of it to
              the user. This functionality is likely broken.
  -O FILE     writes the join result to file FILE. Don't use this for
              performance experiments!
  -r RATE     tuple rate (in tuples/sec) for input stream R.
  -R RATE     tuple rate (in tuples/sec) for input stream S.
  -v          makes our system a tiny bit more verbose.
  -w SIZE     window size for stream R (in seconds).
  -W SIZE     window size for stream S (in seconds).
  -g          turns on GUI mode. This is likely broken, use at your own
              risk! (We had a primitive GUI for this at some point, but
              never maintained it. We might revive this code and write
              up a demo paper for some conference.)
  -i FILE:    When in GUI mode, read commands from this file (usually a
              named pipe). Don't use, the world might collapse if you
              do!
              
B.2 Running Experiments

The above command line options can be used to instantiate a certain
configuration of handshake join and feed data into it. If the
configuration can sustain the applied load, the join output will be
emitted and the program terminates normally. Overload situations will
lead to overruns in the internal FIFO queues. Once FIFOs overflow, the
program will terminate abnormally and print an error message.

For the experiments in the paper, we wrote some Perl scripts that
determine the maximum possible load for every configuration using ``try
and error''. Given upper and lower bounds for the stream rate (known to
be lower and higher than the maximum sustained throughput), the tool
find-max-rate.pl iteratively finds the maximum supported rate for a
given configuration (narrowing down the interval bounds on every
iteration).

Another Perl script, iterate-cores.pl, iterates different
configurations. For each configuration a prediction is made for the
uppoer and lower bounds (based on the previous result and an assumed
scalability) and find-max-rate.pl is invoked.

Inherent to the problem, handshake join needs a certain amount of warmup
time (until both join windows are entirely full). Therefore, make sure
you let the system run long enough (i.e., use enough input tuples per
stream) to obtain meaningful numbers. We always made sure the experiment
runs at least three times as long as the window size.

B.3 Repeating Our Experiments

The main entry point to repeat our experiments is the Perl script
iterate-cores.pl. E.g., throughput numbers for a 15 minute window and 4,
8, 16, and 32 CPU cores can be obtained as follows:

     cd tools
     ./iterate-cores.pl --min 100 --max 2000 -w 900 -W 900 -d 2700 4 8 16 32

The parameters in the end specify the numbers of cores that should be
configured (first run will be with 4 cores, then 8, etc.). Handshake
join instances (or better: the join driver) will be configured to use a
window size of 900 seconds for stream R (-w 900) and a window size of
900 seconds for stream S (-W 900). Input streams will be generated to
last 2700 seconds (note: streams should be significantly longer than the
window size; during the first 900 seconds inherently the join windows
will not have filled up, yet; effectively the system will run for 1800
seconds under full load).

Parameters --min 100 and --max 2000 assume lower and upper bounds for
the throughput of the first configuration (here: 4 cores). (The script
will blindly assume that 100 tuples per second can be sustained, while
2000 cannot.) The script will then, via bisection, try to find the
maximum throughput that the particular configuration can sustain (i.e.,
it'll try a throughput of (100+2000)/2 = 1050 tuples per second; if that
succeeds, it'll try (1050+2000)/2 = 1525 tuples per second, otherwise
it'll try (100+1050)/2 = 575 tuples per second, etc.).

After the throughput of the first configuration has been determined (on
our machine, we were able to sustain 995 tuples/sec on 4 cores), the
script makes an estimate for the throughput that can be sustained in the
next configuration. To that estimate, it adds a safety margin below and
above the estimated throughput, then performs another bisection search
for the new configuration.

At the end, the script will emit a summary like

  SUMMARY:
  32      2806.9921875
  28      2623.2421875
  36      2966.96875
  40      3129.13671875
  12      1720.4140625
  20      2215.765625
  8       1404.6875
  4       995.4140625
  24      2434.65234375
  16      1409.48828125
  44      3290.10546875

B.3.1 Internals of the iterate-cores.pl Script

The iterate-cores.pl script mainly does the iteration over the given
core configurations, and it computes the estimates for configurations
after the first one.

The script will use the Linux script command to produce a number of
detailed log files.

B.3.2 Internals of the find-max-rate.pl Script

iterate-cores.pl depends on another Perl script named find-max-rate.pl.
The latter is the one that does the actual bisection search. It takes
--min, --max, -w, -W, and -d arguments much like iterate-cores.pl. In
addition it takes one single number of cores as the -c argument.

C. Implementation

C.1 Join Driver

During a run of an experiment, the code will first generate all data for
both input streams in memory. Each tuple in those streams is annotated
with a time stamp (determined by the stream rate, with some variations
in the inter-tuple delay). At runtime, the join driver interprets those
time stamps and feeds tuples into handshake join accordingly.

Data generation is implemented in generate_data(). Generated data can be
dumped to a file with a command line option (see above).

To play the materialized streams at runtime, the join driver in
join_driver_exp() uses hj_nanosleep() to implement the inter-tuple
delay. This uses the POSIX real-time function clock_nanosleep() function
if possible. Otherwise, the functionality is simulated with help of
gettimeofday() (not a recommended setting).

C.2 Join Workers

Worker threads are created in setup_workers(). This routine sets up all
the FIFO queues, determines where (i.e., CPU in which NUMA region) to
place each worker, and where to allocate memories. Worker threads will
be spawned and each worker thread gets its own worker context.

C.3 Memory

setup_workers() will allocate memory regions in all participating NUMA
regions. The workers in this region will use the memory in the following
way. The first worker that receives a chunk of data will read this data
from the originating NUMA region. It will do its join processing while
reading the data and, as a side effect, it will physically copy the data
also into the local memory region. Subsequent worker threads in the same
NUMA region will only read this data (and thus do NUMA-local memory
operations only). To enable this functionality, setup_workers() will set
worker_ctx_t::copy_R and worker_ctx_t::copy_S flags in the worker
contexts accordingly.

This is a prototype only, so we keep memory management very simple.
Basically we just allocate enough memory in each NUMA region that we can
hold the entire stream there (our test machine has enough memory so
there's no need to be conservative here). A real-world implementation
would probably allocate memory when data is copied into the NUMA region.
De-allocation is a little bit more tricky, because there are two
destinations where the data (potentially) goes: (a) the next NUMA region
in the processing chain and (b) the result collector.

If you turn on the verbose flag on the command line (-v), you will see
where each worker is being placed and whether or not it copies any data
for R or S.

C.4 Worker Implementation

The join workers themselves are implemented in handshake_join(). The
code there is a relatively straightforward implementation of the pseudo
code in the paper.

If SIMD acceleration was requested during compilation (invoke
./configure with the --enable-simd option), the join kernel is
implemented with help of SIMD intrinsics. See the code regions that are
predicated on the macro ENABLE_SIMD.

Note: If have not tested any of the pathological configurations, such as
a handshake join instance with only a single join worker (that worker
would be the left-most and right-most worker at the same time). Likely
such configurations are broken.

C.5 FIFO Queues

FIFO queues are implemented as lock-free ringbuffers. Sender and
receiver both spin when the queue is full/empty. To avoid deadlock
situations, the sender aborts spinning after some timeout. Since this
indicates a clear overload situation (we use FIFO queues of length 64,
configured via MESSAGE_QUEUE_LENGTH in parameters.h), we usually abort
the program when a FIFO send() times out.

All FIFO code is implemented in ringbuffer.c.

C.6 Result Handling

Besides the FIFO queue to its neighbors, each join worker also has a
message queue to send result tuples to a central result collector. This
collector is implemented in collect_results(). The collector is a
separate thread.

The collector vacuums all its input queues, puts itself to sleep for a
short moment (COLLECT_INTERVAL), then repeats.

If the writing of results to a file has been requested on the command
line (option -O), collect_results() will print all result tuples to that
file. In any case, the collector will report the current result tuple
rate approximately every second.

$Id: README 955 2011-03-17 15:36:40Z jteubner $

Author: Jens Teubner <jens.teubner@inf.ethz.ch>

(c) 2011 ETH Zurich, Systems Group
